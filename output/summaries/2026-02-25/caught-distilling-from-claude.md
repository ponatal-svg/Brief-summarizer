# Caught Distilling from Claude?

**Channel:** Sam Witteveen | **Category:** AI | **Duration:** 13.0m 24.0s | **Language:** en

**Source:** [https://www.youtube.com/watch?v=YemMd6-cM0Q](https://www.youtube.com/watch?v=YemMd6-cM0Q)

---

## The Hook
The topic of model distillation has resurfaced as Anthropic, Google, and OpenAI accuse Chinese AI labs of industrial-scale operations to copy their models, raising questions about intellectual property, model training, and market fairness.

## Key Findings
*   Anthropic, in an article titled "detecting and preventing distillation attacks," accuses Chinese AI labs Deep Seek, Moonshot, and Miniax of running industrial-scale operations. Specifically, these labs are alleged to have created 24,000 fake accounts to extract 16 million exchanges from Anthropic's models. [t=65s]
*   Deep Seek is specifically implicated in 150,000 of these exchanges, allegedly attempting to extract reasoning capabilities across different tasks, use Claude as a reward model for reinforcement learning via rubric-based grading, and create censorship-safe alternatives to policy-sensitive queries by understanding how Claude handles refusals. [t=189s]
*   Moonshot AI, makers of Kimmy K2.5 models, are accused of over 3.4 million exchanges focused on general abilities such as agentic reasoning, tool use, coding, data analysis, computer use, agent development, and computer vision, though the presenter questioned the focus on computer vision given other models' performance. Miniax is accused of the highest amount, over 13 million exchanges, focused on tool use, orchestration, and agentic coding. [t=251s]
*   According to the presenter, a number of observers are pointing out the "kind of coincidence" that all three labs (Anthropic, Google, OpenAI) are raising these concerns simultaneously, potentially targeting the US Congress. If these claims are true, it could partially explain why Mini Max models and Kimmy models have recently shown significant improvements for coding uses. [t=315s]
*   The presenter notes claims from Elon Musk that Anthropic itself is "guilty of stealing training data at massive scales," having paid out a $1.5 billion settlement last year for a copyright dispute over using books. The discussion also touches on the US copyright office's stance that LLM outputs cannot be copyrighted unless humans contribute "sufficient expressive elements." [t=409s]
*   The concept of distillation, detailed in a paper by Jeffrey Hinton, Oriel Vignyals, and Jeff Dean, involves training a smaller model to learn the capabilities of a larger model, either by training on its outputs or its "logits" (the probability distribution of the next token). The presenter suggests it's highly probable that the best models are private and that publicly available models, like "GPG5" (likely a reference to GPT models), are already distillations of even bigger, non-public versions. [t=631s]

## The So What?
This ongoing debate highlights the complex and often contentious landscape of AI model development, intellectual property, and fair use, leaving open questions about the ethics of data sourcing for both large proprietary models and those that seek to learn from them.
